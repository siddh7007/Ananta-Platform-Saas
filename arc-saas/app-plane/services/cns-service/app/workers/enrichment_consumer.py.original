"""
Enrichment Auto-Trigger Consumer

Listens for ``customer.bom.upload_completed`` events and auto-starts
enrichment for organizations that have ``auto_enrichment`` enabled.

Event Flow:
  BOM Upload ‚Üí Upload Workflow ‚Üí customer.bom.upload_completed (RabbitMQ)
  ‚Üí This Consumer ‚Üí Check Org Setting ‚Üí Start Enrichment

Usage:
    python -m app.workers.enrichment_consumer
"""

import os
import sys
import asyncio
import json
import logging
import time
from typing import Dict, Any
import pika
from pika.adapters.blocking_connection import BlockingChannel
from pika.spec import Basic, BasicProperties

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# RabbitMQ Configuration
RABBITMQ_CONFIG = {
    'host': os.getenv('RABBITMQ_HOST', 'localhost'),
    'port': int(os.getenv('RABBITMQ_PORT', '27250')),
    'user': os.getenv('RABBITMQ_USER', 'admin'),
    'password': os.getenv('RABBITMQ_PASS', 'admin123_change_in_production'),
    'virtual_host': os.getenv('RABBITMQ_VHOST', '/'),
    'exchange': 'platform.events',
    'queue': 'bom-enrichment-auto',
    'routing_key': 'customer.bom.upload_completed'  # Listen to upload workflow completion (eliminates race conditions)
}


class EnrichmentAutoTriggerConsumer:
    """
    RabbitMQ consumer for auto-starting enrichment workflows
    """

    def __init__(self):
        self.connection = None
        self.channel = None
        self.temporal_client = None

    def connect_rabbitmq(self, max_retries: int = 10, initial_delay: float = 1.0):
        """
        Connect to RabbitMQ with exponential backoff retry logic

        Args:
            max_retries: Maximum number of connection attempts
            initial_delay: Initial delay in seconds (doubles each retry)
        """
        delay = initial_delay

        for attempt in range(max_retries):
            try:
                logger.info(f"Connecting to RabbitMQ at {RABBITMQ_CONFIG['host']}:{RABBITMQ_CONFIG['port']} (attempt {attempt + 1}/{max_retries})")

                credentials = pika.PlainCredentials(
                    RABBITMQ_CONFIG['user'],
                    RABBITMQ_CONFIG['password']
                )

                parameters = pika.ConnectionParameters(
                    host=RABBITMQ_CONFIG['host'],
                    port=RABBITMQ_CONFIG['port'],
                    virtual_host=RABBITMQ_CONFIG['virtual_host'],
                    credentials=credentials,
                    heartbeat=600,
                    blocked_connection_timeout=300
                )

                self.connection = pika.BlockingConnection(parameters)
                self.channel = self.connection.channel()

                # Declare exchange (idempotent)
                self.channel.exchange_declare(
                    exchange=RABBITMQ_CONFIG['exchange'],
                    exchange_type='topic',
                    durable=True
                )

                # Declare queue (idempotent)
                self.channel.queue_declare(
                    queue=RABBITMQ_CONFIG['queue'],
                    durable=True,
                    arguments={
                        'x-message-ttl': 3600000,  # 1 hour
                        'x-max-length': 5000,
                    }
                )

                # Bind queue to exchange
                self.channel.queue_bind(
                    exchange=RABBITMQ_CONFIG['exchange'],
                    queue=RABBITMQ_CONFIG['queue'],
                    routing_key=RABBITMQ_CONFIG['routing_key']
                )

                logger.info(f"‚úÖ Connected to RabbitMQ")
                logger.info(f"üìÆ Listening on queue: {RABBITMQ_CONFIG['queue']}")
                logger.info(f"üîë Routing key: {RABBITMQ_CONFIG['routing_key']}")
                return  # Success!

            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"‚ùå Failed to connect to RabbitMQ after {max_retries} attempts: {e}")
                    raise

                logger.warning(f"‚ö†Ô∏è  RabbitMQ connection failed (attempt {attempt + 1}/{max_retries}): {e}")
                logger.info(f"‚è≥ Retrying in {delay:.1f}s...")
                time.sleep(delay)
                delay = min(delay * 2, 60)  # Exponential backoff, max 60s

    async def connect_temporal(self, max_retries: int = 10, initial_delay: float = 1.0):
        """
        Connect to Temporal with exponential backoff retry logic

        Args:
            max_retries: Maximum number of connection attempts
            initial_delay: Initial delay in seconds (doubles each retry)
        """
        from temporalio.client import Client

        temporal_host = os.getenv('TEMPORAL_HOST', 'localhost:7233')
        delay = initial_delay

        for attempt in range(max_retries):
            try:
                logger.info(f"Connecting to Temporal at {temporal_host} (attempt {attempt + 1}/{max_retries})")

                self.temporal_client = await Client.connect(temporal_host)

                logger.info("‚úÖ Connected to Temporal")
                return  # Success!

            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"‚ùå Failed to connect to Temporal after {max_retries} attempts: {e}")
                    raise

                logger.warning(f"‚ö†Ô∏è  Temporal connection failed (attempt {attempt + 1}/{max_retries}): {e}")
                logger.info(f"‚è≥ Retrying in {delay:.1f}s...")
                await asyncio.sleep(delay)
                delay = min(delay * 2, 60)  # Exponential backoff, max 60s

    async def check_auto_enrichment_enabled(self, tenant_id: str) -> bool:
        """
        Check if organization has auto-enrichment enabled

        Args:
            tenant_id: Organization ID

        Returns:
            True if auto-enrichment is enabled, False otherwise
        """
        from app.models.dual_database import get_dual_database
        from sqlalchemy import text

        try:
            dual_db = get_dual_database()
            db = next(dual_db.get_session("supabase"))

            query = text("""
                SELECT auto_enrichment
                FROM organizations
                WHERE id = :tenant_id
            """)

            result = db.execute(query, {"tenant_id": tenant_id})
            row = result.fetchone()

            if not row:
                logger.warning(f"Organization not found: {tenant_id}")
                return False

            auto_enrichment = row[0]
            logger.info(f"Organization {tenant_id}: auto_enrichment = {auto_enrichment}")
            return bool(auto_enrichment)

        except Exception as e:
            logger.error(f"Error checking auto-enrichment setting: {e}", exc_info=True)
            return False

    async def handle_bom_uploaded_event(self, event_data: Dict[str, Any], priority: int = 5) -> tuple[bool, str]:
        """
        Handle ``customer.bom.upload_completed`` event.

        Checks if organization has auto-enrichment enabled.
        If yes, starts enrichment workflow automatically.
        If no, does nothing (user must manually trigger).

        Args:
            event_data: Event payload from RabbitMQ message
            priority: Priority level (1-9, from RabbitMQ message properties)

        Returns:
            Tuple of (success: bool, error_type: str)
            error_type: 'transient' (should requeue) or 'permanent' (should drop)
        """
        try:
            logger.info(
                f"üì• Received BOM upload_completed event: {event_data.get('bom_id')} "
                f"(priority={priority})"
            )

            # Extract event data
            bom_upload_id = event_data.get('bom_id')  # bom_uploads.id
            tenant_id = event_data.get('tenant_id')
            project_id = event_data.get('project_id')
            user_id = event_data.get('user_id')
            filename = event_data.get('filename')

            if not bom_upload_id or not tenant_id:
                logger.error(f"Invalid event data: missing required fields")
                return (False, 'permanent')  # Malformed message, don't requeue

            # Check if organization has auto-enrichment enabled
            auto_enrichment_enabled = await self.check_auto_enrichment_enabled(tenant_id)

            if not auto_enrichment_enabled:
                logger.info(
                    f"‚è∏Ô∏è  Auto-enrichment DISABLED for org {tenant_id}. "
                    f"User must manually start enrichment."
                )
                return (True, '')  # Ack message but don't process

            logger.info(f"‚úÖ Auto-enrichment ENABLED for org {tenant_id}")

            # No delay needed - event is published AFTER upload workflow completes
            # and boms/bom_line_items are already created in database

            # Query Supabase for boms record created by upload workflow
            from app.models.dual_database import get_dual_database
            from sqlalchemy import text

            dual_db = get_dual_database()
            db = next(dual_db.get_session("supabase"))

            # Find boms record
            query = text("""
                SELECT id, status, component_count, metadata
                FROM boms
                WHERE tenant_id = :tenant_id
                  AND metadata->>'bom_upload_id' = :bom_upload_id
                ORDER BY created_at DESC
                LIMIT 1
            """)

            result = db.execute(query, {
                "tenant_id": tenant_id,
                "bom_upload_id": bom_upload_id
            })
            bom = result.fetchone()

            if not bom:
                logger.error(f"‚ùå BOM not found for upload: {bom_upload_id}")
                return (False, 'transient')  # Race condition - requeue for retry

            bom_id = bom[0]
            status = bom[1]
            component_count = bom[2]
            metadata = bom[3] or {}

            logger.info(f"Found BOM {bom_id}: status={status}, items={component_count}")

            # Check if upload completed successfully
            if status != 'completed':
                logger.warning(f"‚è∏Ô∏è  BOM {bom_id} not ready (status={status}), skipping")
                return (False, 'transient')  # Race condition - requeue for retry

            # Check if already enriching
            if status in ['enriching', 'enriched']:
                logger.info(f"‚è© BOM {bom_id} already {status}, skipping")
                return (True, '')  # Already handled, acknowledge message

            # Count pending line items
            count_query = text("""
                SELECT COUNT(*)
                FROM bom_line_items
                WHERE bom_id = :bom_id
                  AND enrichment_status = 'pending'
            """)
            result = db.execute(count_query, {"bom_id": bom_id})
            pending_count = result.fetchone()[0]

            if pending_count == 0:
                logger.warning(f"‚ö†Ô∏è  No pending items to enrich for BOM {bom_id}")
                return (False, 'permanent')  # No items to process, don't requeue

            logger.info(f"üöÄ Auto-starting enrichment: BOM {bom_id} ({pending_count} items)")

            # Import workflow
            from datetime import timedelta
            from temporalio.common import WorkflowIDReusePolicy
            from app.workflows.bom_enrichment import (
                BOMEnrichmentWorkflow,
                BOMEnrichmentRequest
            )
            from app.config import settings

            # Create enrichment request
            enrichment_request = BOMEnrichmentRequest(
                job_id=bom_id,
                bom_id=bom_id,
                tenant_id=tenant_id,
                project_id=project_id,
                total_items=pending_count,
                source='customer',  # Customer Portal uploads
                user_id=user_id,
                workflow_id=f"bom-enrichment-{bom_id}"
            )

            # Start Temporal enrichment workflow
            workflow_id = f"bom-enrichment-{bom_id}"

            handle = await self.temporal_client.start_workflow(
                BOMEnrichmentWorkflow.run,
                enrichment_request,
                id=workflow_id,
                task_queue=settings.temporal_task_queue,
                # Workflow discoverability and searchability
                memo={
                    'tenant_id': tenant_id,
                    'bom_id': bom_id,
                    'project_id': project_id or '',
                    'user_id': user_id or '',
                    'source': 'customer',
                    'auto_triggered': 'true'
                },
                # ID reuse policy: reject duplicate workflow IDs
                id_reuse_policy=WorkflowIDReusePolicy.REJECT_DUPLICATE,
                # Timeouts (prevent runaway workflows)
                execution_timeout=timedelta(hours=24),  # Max 24 hours for entire workflow
                run_timeout=timedelta(hours=12)  # Max 12 hours per run (allows retries)
            )

            logger.info(f"‚úÖ Enrichment workflow started: {workflow_id}")
            logger.info(f"   Run ID: {handle.first_execution_run_id}")

            # Update boms status
            update_query = text("""
                UPDATE boms
                SET
                    status = 'enriching',
                    metadata = jsonb_set(
                        COALESCE(metadata, '{}'::jsonb),
                        '{enrichment_workflow_id}',
                        to_jsonb(:workflow_id::text)
                    ),
                    metadata = jsonb_set(
                        metadata,
                        '{enrichment_run_id}',
                        to_jsonb(:run_id::text)
                    ),
                    metadata = jsonb_set(
                        metadata,
                        '{auto_enrichment}',
                        'true'::jsonb
                    ),
                    metadata = jsonb_set(
                        metadata,
                        '{enrichment_started_at}',
                        to_jsonb(NOW()::text)
                    )
                WHERE id = :bom_id
            """)

            db.execute(update_query, {
                "bom_id": bom_id,
                "workflow_id": workflow_id,
                "run_id": handle.first_execution_run_id
            })
            db.commit()

            logger.info(f"‚úÖ Auto-enrichment initiated for BOM {bom_id}")

            # Publish enrichment started event
            try:
                from shared.event_bus import EventPublisher
                EventPublisher.customer_bom_enrichment_started(
                    job_id=bom_id,
                    bom_id=bom_id,
                    total_items=pending_count
                )
            except Exception as e:
                logger.warning(f"Failed to publish enrichment started event: {e}")

            return (True, '')  # Success

        except Exception as e:
            logger.error(f"‚ùå Error handling BOM uploaded event: {e}", exc_info=True)
            return (False, 'transient')  # Unknown error - retry

    def callback(self, ch: BlockingChannel, method: Basic.Deliver, properties: BasicProperties, body: bytes):
        """RabbitMQ message callback with intelligent requeue logic"""
        try:
            event_data = json.loads(body)
            priority = properties.priority or 5

            logger.info(f"Processing message: {event_data.get('event_type', 'unknown')}")

            # Run async handler
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            success, error_type = loop.run_until_complete(
                self.handle_bom_uploaded_event(event_data, priority)
            )

            loop.close()

            if success:
                ch.basic_ack(delivery_tag=method.delivery_tag)
                logger.info("‚úÖ Message acknowledged")
            else:
                # Intelligent requeue based on error type
                should_requeue = (error_type == 'transient')
                ch.basic_nack(delivery_tag=method.delivery_tag, requeue=should_requeue)

                if should_requeue:
                    logger.info(f"‚è≠Ô∏è  Message requeued for retry (error_type={error_type})")
                else:
                    logger.warning(f"üíÄ Message dropped (error_type={error_type})")

        except Exception as e:
            logger.error(f"‚ùå Error processing message: {e}", exc_info=True)
            # Unknown exception - requeue for safety
            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
            logger.info("‚è≠Ô∏è  Message requeued due to unexpected error")

    async def start(self):
        """Start consuming messages"""
        logger.info("üöÄ Starting Enrichment Auto-Trigger Consumer...")
        logger.info("   Mode: Hybrid (per-organization toggle)")

        # Connect to services
        self.connect_rabbitmq()
        await self.connect_temporal()

        logger.info("")
        logger.info("üìã Behavior:")
        logger.info("   ‚Ä¢ Organizations with auto_enrichment=true ‚Üí Auto-start enrichment")
        logger.info("   ‚Ä¢ Organizations with auto_enrichment=false ‚Üí Manual trigger required")
        logger.info("")

        # Start consuming
        self.channel.basic_qos(prefetch_count=1)
        self.channel.basic_consume(
            queue=RABBITMQ_CONFIG['queue'],
            on_message_callback=self.callback
        )

        logger.info("üîÑ Consumer started. Waiting for BOM upload events...")
        self.channel.start_consuming()


async def main():
    """Main entry point"""
    consumer = EnrichmentAutoTriggerConsumer()

    try:
        await consumer.start()

    except KeyboardInterrupt:
        logger.info("‚ö†Ô∏è  Consumer shutdown requested")

    except Exception as e:
        logger.error(f"‚ùå Consumer error: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    asyncio.run(main())
