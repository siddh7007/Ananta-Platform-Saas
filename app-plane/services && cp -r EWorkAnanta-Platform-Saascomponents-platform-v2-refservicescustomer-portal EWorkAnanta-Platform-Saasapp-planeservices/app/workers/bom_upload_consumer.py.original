"""
BOM Upload Event Consumer - Unified for CBP and CNS

Listens for BOTH 'customer.bom.uploaded' AND 'cns.bom.bulk_uploaded' events
from RabbitMQ and starts Temporal workflows to process BOM uploads.

Supports:
- Customer Portal (CBP) uploads: priority=7 (HIGH)
- CNS Bulk uploads: priority=5 (MEDIUM)
- Single Temporal scheduler handles both with varying priorities

Event Flow:
  CBP/CNS ‚Üí Supabase bom_uploads ‚Üí RabbitMQ Event ‚Üí This Consumer ‚Üí Temporal Workflow

Event Formats:
  CBP: customer.bom.uploaded (priority=7)
  {
    "bom_id": "uuid",
    "project_id": "uuid",
    "tenant_id": "uuid",
    "filename": "bom.csv",
    "status": "pending",
    "total_rows": 100,
    "timestamp": "2025-11-10T12:34:56Z"
  }

  CNS: cns.bom.bulk_uploaded (priority=5)
  {
    "bom_id": "uuid",
    "tenant_id": "uuid",
    "admin_id": "uuid",
    "filename": "bulk.csv",
    "file_size": 12345,
    "s3_key": "tenant_id/upload_id/file.csv",
    "s3_bucket": "uploads",
    "storage_backend": "minio"
  }

Usage:
    python -m app.workers.bom_upload_consumer
"""

import os
import sys
import asyncio
import json
import logging
from typing import Dict, Any
import pika
from pika.adapters.blocking_connection import BlockingChannel
from pika.spec import Basic, BasicProperties

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# RabbitMQ Configuration
RABBITMQ_CONFIG = {
    'host': os.getenv('RABBITMQ_HOST', 'localhost'),
    'port': int(os.getenv('RABBITMQ_PORT', '27250')),
    'user': os.getenv('RABBITMQ_USER', 'admin'),
    'password': os.getenv('RABBITMQ_PASS', 'admin123_change_in_production'),
    'virtual_host': os.getenv('RABBITMQ_VHOST', '/'),
    'exchange': 'platform.events',
    'queue': 'bom-upload-processing',
    # Listen to BOTH routing keys
    'routing_keys': [
        'customer.bom.uploaded',    # CBP uploads (priority=7)
        'cns.bom.bulk_uploaded'      # CNS bulk uploads (priority=5)
    ]
}


class BOMUploadConsumer:
    """
    RabbitMQ consumer for BOM upload events
    """

    def __init__(self):
        self.connection = None
        self.channel = None
        self.temporal_client = None

    def connect_rabbitmq(self):
        """Connect to RabbitMQ"""
        logger.info(f"Connecting to RabbitMQ at {RABBITMQ_CONFIG['host']}:{RABBITMQ_CONFIG['port']}")

        credentials = pika.PlainCredentials(
            RABBITMQ_CONFIG['user'],
            RABBITMQ_CONFIG['password']
        )

        parameters = pika.ConnectionParameters(
            host=RABBITMQ_CONFIG['host'],
            port=RABBITMQ_CONFIG['port'],
            virtual_host=RABBITMQ_CONFIG['virtual_host'],
            credentials=credentials,
            heartbeat=600,
            blocked_connection_timeout=300
        )

        self.connection = pika.BlockingConnection(parameters)
        self.channel = self.connection.channel()

        # Declare exchange (idempotent)
        self.channel.exchange_declare(
            exchange=RABBITMQ_CONFIG['exchange'],
            exchange_type='topic',
            durable=True
        )

        # Declare queue (idempotent)
        self.channel.queue_declare(
            queue=RABBITMQ_CONFIG['queue'],
            durable=True,
            arguments={
                'x-message-ttl': 86400000,  # 24 hours
                'x-max-length': 10000,  # Max 10k messages
            }
        )

        # Bind queue to exchange for BOTH routing keys
        for routing_key in RABBITMQ_CONFIG['routing_keys']:
            self.channel.queue_bind(
                exchange=RABBITMQ_CONFIG['exchange'],
                queue=RABBITMQ_CONFIG['queue'],
                routing_key=routing_key
            )
            logger.info(f"üîë Bound to routing key: {routing_key}")

        logger.info(f"‚úÖ Connected to RabbitMQ")
        logger.info(f"üìÆ Listening on queue: {RABBITMQ_CONFIG['queue']}")
        logger.info(f"üîÑ Handling both CBP (priority=7) and CNS (priority=5) uploads")

    async def connect_temporal(self):
        """Connect to Temporal"""
        from temporalio.client import Client

        temporal_host = os.getenv('TEMPORAL_HOST', 'localhost:7233')
        logger.info(f"Connecting to Temporal at {temporal_host}")

        self.temporal_client = await Client.connect(temporal_host)

        logger.info("‚úÖ Connected to Temporal")

    async def handle_bom_uploaded_event(self, event_data: Dict[str, Any], event_type: str, priority: int = 5):
        """
        Handle BOM uploaded events (both CBP and CNS) by starting Temporal workflow

        Args:
            event_data: Event payload from RabbitMQ message
            event_type: Event routing key (customer.bom.uploaded or cns.bom.bulk_uploaded)
            priority: Priority level (1-9, from RabbitMQ message properties)
        """
        try:
            # Determine upload source
            is_cns_bulk = event_type == 'cns.bom.bulk_uploaded'
            source_label = "CNS Bulk" if is_cns_bulk else "CBP"

            logger.info(
                f"üì• Received {source_label} upload event: {event_data.get('upload_id')} "
                f"(priority={priority}, source={event_type})"
            )

            # Extract common fields
            bom_upload_id = event_data.get('upload_id')  # bom_uploads.id (NOT bom_id from boms table!)
            tenant_id = event_data.get('tenant_id')
            filename = event_data.get('filename')

            if not bom_upload_id or not tenant_id:
                logger.error(f"Invalid event data: missing required fields")
                return False

            # Extract source-specific fields
            if is_cns_bulk:
                # CNS bulk upload - has S3 location
                s3_key = event_data.get('s3_key')
                s3_bucket = event_data.get('s3_bucket')
                admin_id = event_data.get('admin_id')
                project_id = event_data.get('project_id')  # Optional for CNS

                logger.info(f"   S3 Location: s3://{s3_bucket}/{s3_key}")
                logger.info(f"   Admin: {admin_id}")
            else:
                # CBP upload - already parsed client-side
                project_id = event_data.get('project_id')
                total_rows = event_data.get('total_rows', 0)

                logger.info(f"   Project: {project_id}")
                logger.info(f"   Total rows: {total_rows}")

            # Import workflow class
            from app.workflows.bom_upload_workflow import (
                BOMUploadProcessWorkflow,
                BOMUploadProcessRequest
            )

            # Create workflow request with priority and source info
            request = BOMUploadProcessRequest(
                bom_upload_id=bom_upload_id,
                tenant_id=tenant_id,
                project_id=project_id,
                filename=filename,
                priority=priority  # Pass priority to workflow
            )

            # Start Temporal workflow
            workflow_id = f"bom-upload-{bom_upload_id}"
            logger.info(
                f"üöÄ Starting Temporal workflow: {workflow_id} "
                f"(priority={priority}, source={source_label})"
            )

            # Start workflow with priority metadata
            handle = await self.temporal_client.start_workflow(
                BOMUploadProcessWorkflow.run,
                request,
                id=workflow_id,
                task_queue="bom-upload-processing",
                # Add metadata for observability
                memo={
                    'source': source_label,
                    'priority': priority,
                    'event_type': event_type,
                    'tenant_id': tenant_id
                }
            )

            logger.info(f"‚úÖ Workflow started: {workflow_id}")
            logger.info(f"   Run ID: {handle.first_execution_run_id}")
            logger.info(f"   Priority: {priority} ({source_label})")
            logger.info(f"   Task Queue: bom-upload-processing (shared scheduler)")

            return True

        except Exception as e:
            logger.error(
                f"‚ùå Error handling BOM uploaded event: {e}",
                exc_info=True,
                extra={
                    'event_type': event_type,
                    'bom_id': event_data.get('bom_id'),
                    'priority': priority
                }
            )
            return False

    def on_message(self, channel: BlockingChannel, method: Basic.Deliver, properties: BasicProperties, body: bytes):
        """
        Callback when message is received from RabbitMQ

        Handles both CBP and CNS events with different priorities
        """
        try:
            # Parse message
            message = json.loads(body)
            event_type = message.get('event_type', 'unknown')
            routing_key = method.routing_key

            logger.info(f"üì® Received message: {event_type} (routing: {routing_key})")

            # Extract priority from RabbitMQ message properties
            # Priority: 1-9 scale (9=highest), default=5
            # CBP uploads typically have priority=7
            # CNS bulk uploads typically have priority=5
            priority = properties.priority if properties.priority is not None else 5

            # Log source and priority
            source = "CBP" if routing_key == 'customer.bom.uploaded' else "CNS"
            logger.info(f"   Source: {source}")
            logger.info(f"   Priority: {priority}")

            # Handle event asynchronously in a separate thread
            # This avoids event loop conflicts with Pika's blocking connection
            import concurrent.futures
            import threading

            def run_async_in_thread():
                """Run async function in a new thread with its own event loop"""
                return asyncio.run(
                    self.handle_bom_uploaded_event(
                        event_data=message,
                        event_type=routing_key,
                        priority=priority
                    )
                )

            # Execute in thread pool to avoid event loop conflicts
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(run_async_in_thread)
                success = future.result(timeout=300)  # 5 minute timeout

            if success:
                # Acknowledge message
                channel.basic_ack(delivery_tag=method.delivery_tag)
                logger.info(f"‚úÖ Message acknowledged")
            else:
                # Reject and requeue (will retry)
                channel.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
                logger.warning(f"‚ö†Ô∏è  Message rejected and requeued")

        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in message: {e}")
            # Reject without requeue (bad message)
            channel.basic_nack(delivery_tag=method.delivery_tag, requeue=False)

        except Exception as e:
            logger.error(f"Error processing message: {e}", exc_info=True)
            # Reject and requeue (temporary error)
            channel.basic_nack(delivery_tag=method.delivery_tag, requeue=True)

    async def start_consuming(self):
        """
        Start consuming messages from RabbitMQ
        """
        logger.info("=" * 80)
        logger.info("üöÄ BOM Upload Event Consumer Starting")
        logger.info("=" * 80)

        try:
            # Connect to services
            self.connect_rabbitmq()
            await self.connect_temporal()

            # Set QoS (prefetch count)
            self.channel.basic_qos(prefetch_count=1)

            # Start consuming
            self.channel.basic_consume(
                queue=RABBITMQ_CONFIG['queue'],
                on_message_callback=self.on_message,
                auto_ack=False  # Manual acknowledgment
            )

            logger.info("‚úÖ Consumer ready. Waiting for messages...")
            logger.info("   Press Ctrl+C to stop")

            # Block and consume messages
            self.channel.start_consuming()

        except KeyboardInterrupt:
            logger.info("\n‚ö†Ô∏è  Consumer stopped by user")
            self.stop()

        except Exception as e:
            logger.error(f"‚ùå Consumer error: {e}", exc_info=True)
            self.stop()
            raise

    def stop(self):
        """Stop consumer and close connections"""
        logger.info("üõë Stopping consumer...")

        if self.channel:
            self.channel.stop_consuming()
            self.channel.close()

        if self.connection:
            self.connection.close()

        logger.info("‚úÖ Consumer stopped")


async def main():
    """Main entry point"""
    consumer = BOMUploadConsumer()
    await consumer.start_consuming()


if __name__ == '__main__':
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\nüëã Goodbye!")
        sys.exit(0)
